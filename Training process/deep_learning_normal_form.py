# -*- coding: utf-8 -*-
"""Deep_learning_Normal_form.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/192tzVMNQNtF9gRTTgAponivr_p9K4OkN
"""

from keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import fsolve
from scipy.ndimage import gaussian_filter1d
import random

from google.colab import files #upload excel file
uploaded = files.upload()

Time_series = np.load('Time_series_r=0.5.npy')

from google.colab import files #upload excel file
uploaded = files.upload()

Label = np.load('Label_r=0.5.npy')

Label[16]

Train_fraction = 0.9
Valid_fraction = 0.05
test_fraction = 0.05
Number_gather = 20000

Number_train = int(Number_gather*Train_fraction)
Number_valid = int(Number_gather*Valid_fraction)
Number_test = int(Number_gather*test_fraction)

Train = Time_series[0:Number_train,:]
Train_target = Label[0:Number_train]

Validation = Time_series[Number_train:Number_train+Number_valid,:]
Validation_target = Label[Number_train:Number_train+Number_valid]

Test = Time_series[Number_train+Number_valid:Number_gather,:]
Test_target = Label[Number_train+Number_valid:Number_gather]

from keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint
pool_size_param = 2
learning_rate_param = 0.01 #used to be 0.0005 0.001 0.005 0.01
batch_param = 128
dropout_percent = 0.05 #0.1
filters_param = 50 #used to be 50
mem_cells = 50 #100
mem_cells2 = 10 #50
kernel_size_param = 10 #for precomp result it used to be 5 #for comp used to be 10
epoch_param = 200 #used to be 100
initializer_param = 'lecun_normal'

model = Sequential()
model.add(Conv1D(filters=filters_param, kernel_size=kernel_size_param, activation='relu', padding='same',input_shape=(1000,1),kernel_initializer = initializer_param))
model.add(Conv1D(filters=2*filters_param, kernel_size=kernel_size_param, activation='relu', padding='same'))
model.add(Dropout(dropout_percent))
model.add(MaxPooling1D(pool_size=pool_size_param, strides=2, padding='valid'))
model.add(LSTM(mem_cells, return_sequences=True, kernel_initializer = initializer_param))
model.add(Dropout(dropout_percent))
model.add(LSTM(mem_cells2,kernel_initializer = initializer_param))
model.add(Dropout(dropout_percent))
model.add(Dense(4, activation='softmax',kernel_initializer = initializer_param)) #Dense used to be 4
model_name = 'best_model.pkl'

# Set up optimiser
adam = Adam(learning_rate=learning_rate_param)
chk = ModelCheckpoint(model_name, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
early_stop = EarlyStopping(monitor='val_accuracy', patience=10, mode='max', verbose=1)
model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy', 'sparse_categorical_accuracy'])
#model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

# Train model
history = model.fit(Train, Train_target, epochs=epoch_param, batch_size=batch_param, callbacks=[chk, early_stop], validation_data=(Validation, Validation_target))

#history = model.fit(Train, Train_target, epochs=epoch_param, batch_size=batch_param, callbacks=[chk],validation_data=(Validation,Validation_target))

model = load_model(model_name)

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training Accuracy')
plt.plot(history.history['val_loss'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()



from sklearn.metrics import accuracy_score

#test_preds = model.predict_classes(test)

predict_x=model.predict(Test)
test_preds=np.argmax(predict_x,axis=1)

print("accuracy score: ",accuracy_score(Test_target, test_preds))

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report

print(classification_report(Test_target, test_preds, digits=3))

print(history.history['accuracy'])
print(history.history['val_accuracy'])
print(history.history['loss'])
print(history.history['val_loss'])
print("F1 score:",f1_score(Test_target, test_preds, average='macro'))
print("Precision: ",precision_score(Test_target, test_preds, average="macro"))
print("Recall: ",recall_score(Test_target, test_preds, average="macro"))
print("Confusion matrix: \n",confusion_matrix(Test_target, test_preds))

model.save('Normal_form_CNN_LSTM_r=0.5.h5')

from google.colab import files
files.download('Normal_form_CNN_LSTM_r=0.5.h5')

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv1D, Concatenate, MaxPooling1D, Flatten, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

def inception_block(inputs, filters):
    conv1x1 = Conv1D(filters, kernel_size=5, activation='relu', padding='same')(inputs)
    conv3x3 = Conv1D(filters, kernel_size=10, activation='relu', padding='same')(inputs)
    conv5x5 = Conv1D(filters, kernel_size=30, activation='relu', padding='same')(inputs)

    max_pool1x1 = MaxPooling1D(pool_size=3, strides=1, padding='same')(conv1x1)
    max_pool3x3 = MaxPooling1D(pool_size=3, strides=1, padding='same')(conv3x3)
    max_pool5x5 = MaxPooling1D(pool_size=3, strides=1, padding='same')(conv5x5)

    concatenated = Concatenate(axis=-1)([max_pool1x1, max_pool3x3, max_pool5x5])

    return concatenated
input_shape = (1000,1)  # Define your sequence length and number of channels
num_classes =4
Train_target_encoded = to_categorical(Train_target, num_classes)
Validation_target_encoded = to_categorical(Validation_target, num_classes)
Test_target_encoded = to_categorical(Test_target, num_classes)
# Define model architecture
inputs = Input(shape=input_shape)
inception_output = inception_block(inputs, filters=32)
flatten_output = Flatten()(inception_output)
output = Dense(num_classes, activation='softmax')(flatten_output)

model = Model(inputs=inputs, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(patience=20, restore_best_weights=True)

history = model.fit(Train, Train_target_encoded, validation_data=(Validation, Validation_target_encoded), epochs=200, batch_size=20, callbacks=[early_stopping])
"""
val_loss, val_accuracy = model.evaluate(Validation, Validation_target)
print("Validation Loss:", val_loss)
print("Validation Accuracy:", val_accuracy)

test_loss, test_accuracy = model.evaluate(Test, Test_target)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
"""

from sklearn.metrics import accuracy_score

#test_preds = model.predict_classes(test)

predict_x=model.predict(Test)
test_preds=np.argmax(predict_x,axis=1)

print("accuracy score: ",accuracy_score(Test_target, test_preds))

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report

print(classification_report(Test_target, test_preds, digits=3))

print(history.history['accuracy'])
print(history.history['val_accuracy'])
print(history.history['loss'])
print(history.history['val_loss'])
print("F1 score:",f1_score(Test_target, test_preds, average='macro'))
print("Precision: ",precision_score(Test_target, test_preds, average="macro"))
print("Recall: ",recall_score(Test_target, test_preds, average="macro"))
print("Confusion matrix: \n",confusion_matrix(Test_target, test_preds))

model.save('Normal_form_Inception.h5')

from google.colab import files
files.download('Normal_form_Inception.h5')